---
title: "Examples"
author: "Martínez and Salibian"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Examples}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
## About this vignette

In this vignette, we will show with a real data example and some contaminated responses how a robust backfitting procedure can be easily implemented.

The `R` package `RBF` (available on CRAN [here](https://cran.r-project.org/package=RBF)) 
implements the robust back-fitting algorithm as proposed by
Boente, Martinez and Salibian-Barrera  in 

> Boente G, Martinez A, Salibian-Barrera M. (2017) Robust estimators for 
> additive models using backfitting. Journal of Nonparametric Statistics. Taylor 
> & Francis; 29, 744-767.
> [DOI: 10.1080/10485252.2017.1369077](https://doi.org/10.1080/10485252.2017.1369077)

This repository contains a development version of `RBF`
which may differ slightly from the one available on CRAN
(until the CRAN version is updated appropriately). 

The package in this repository can be installed from within `R` by using the following code (assuming the [devtools](https://cran.r-project.org/package=devtools)) package is available:
```R
devtools::install_github("msalibian/RBF")
```
and charged in the `R` session by the following command
```{r library}
library("RBF")
```
Now that the `R` package is downloaded, we can now start to see how this procedure works.

We consider the Boston house price data of Harrinson and Rubinfeld (1978). This dataset was used as an example to model an additive model by Härdle et a. (2004). File `housing.csv` available in this GitHub repository contains $n=506$ observations for the census districts of the Boston metropolitan area. As in this book, we selected the following ten explanatory variables

$X_1$: per capita crime rate by town,

$X_2$: proportion of non-retail business acres per town,

$X_3$: nitric oxides concentration (parts per 10 million),

$X_4$: avarage number of rooms per dwelling,

$X_5$: proportion of owner-occupied units built prior to 1940,

$X_6$: weighted distances to five Boston employment centers,

$X_7$: full-value property tax rate per $10,000,

$X_8$: pupil-teacher ratio by town,

$X_9$: $1,000(Bk-0.63)^2$ where $Bk$ is the proportion of people of Afrom American descent by town,

$X_{10}$: percent lower status of population,

and the $Y$ response variable is the median value of the owner-occupied homes in 1,000 USD. Then, the additive model assumed is
$Y= \mu+ \sum_{j=1}^{10} g_j(\log(X_j))+ \epsilon.$


```{r read the dataset}
datos <- read.csv("housing.csv", header = FALSE, sep="")
x1 <- log(datos$V1)
x2 <- log(datos$V3)
x3 <- log(datos$V5)
x4 <- log(datos$V6)
x5 <- log(datos$V7)
x6 <- log(datos$V8)
x7 <- log(datos$V10)
x8 <- log(datos$V11)
x9 <- log(datos$V12)
x10 <- log(datos$V13)
y <- datos$V14
```

Recall that bandwidth selection is an important issue when considering any kernel-based method. As in Härdle et al. (2004), bandwidths a $h_j$, for $1 \leq j \leq 10$ are chosen proportional to its standard deviation ($h_j=0.5 \hat{\sigma}_j$).
```{r bandwidths}
X <- cbind(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10)
colnames(X) <- NULL #HAY QUE ARREGLAR LOS NOMBRES CUANDO VIENEN CON NOMBRES EN EL .R DEL PAQUETE
bandw <- (1/2)*apply(X,2,sd) 
bandw
```

We will now compute the robust backfitting estimator in a similar way to how it's done in other well-known packages: using a `formula`. This argument and algo the `windows` argument are the only two arguments needed to obtain the robust fit since the other arguments have a default value. Then, we can just obtain our robust fit by doing
```{r robust fit}
robust.fit <- backf.rob(y~X, windows=bandw)
```
Covariates can be included as a matrix as before but also separated with `+`, that is `y~x1+x2+x3+x4+x5+x6+x7+x8+x9+x10`.

Another important argument is the `type` argument which calls to the loss function used in the robust procedure. It's default value is the Huber's loss function (`type='Huber'`). The other option is to use the Tukey's bisquared loss function (`type='Tukey'`).

Now, we can see how the robust fit performed using the `summary` method:
```{r summary}
summary(robust.fit)
```
and we can obtain from the method `plot` the plots of the partial residuals together with the estimated curves. Once the fit is done and estimations of the constant $\mu$ and the additive functions $g_j$ are obtained, the partial residual associated to the  $j$th covariate for the $i$th observation is defined as $$R_{ij}=Y_i-\hat{\mu}-\sum_{k\neq i}\hat{g}_k(X_{ik}).$$ Then, points of the partial residuals obtained for the $j$th covariate are of the form $(X_{ij},R_{ij})$.
```{r plot}
plot(robust.fit)
```


If you want a prediction of the additive functions $\hat{g}_j$ at a certain point, you can add it in the argument `point` when doing the fit.
```{r prediction}
po <- colMeans(X)
robust.fit1 <- backf.rob(y~X, windows = bandw, point = po)
robust.fit1$prediction
```

In order to analyze the sensitivity of the robust estimator, we will contaminate the responses of a 1% of the observations.
```{r outliers}
ynew <- y
#ynew[c(4,89,197,198,199,200,299,291,301,350)]<- rep(400,10)
ynew[1:5]<- rep(400,5)
```
Then, if the recalculate the robust fit, we obtain similar curves to what was obtained with the original data set.
```{r robustplotswithoutliers}
robust.fit.new <- backf.rob(ynew~X, windows = bandw, point = po)
summary(robust.fit.new)
robust.fit.new$prediction
plot(robust.fit.new)
```

Since the differences between the two estimations cannot be appreciated, we will plot both robust fits for all the ten covariates. In green and dashed lines the robust estimator computed with the original data set, and in blue and solid lines the robust estimator computed with the contaminated data set.

```{r robustplots2, warning=FALSE}
for(j in 1:10) {
  name.x <- bquote(paste('x')[.(j)]) 
  name.y <- bquote(paste(hat('g')[.(j)]))
  oo <- order(X[,j])
  plot(X[oo,j], robust.fit.new$g.matrix[oo,j], type="l", lwd=5, col='blue', lty=1, 
       xlab=name.x, ylab=name.y)
  lines(X[oo,j], robust.fit$g.matrix[oo,j], lwd=5, col='green', lty=2)
}
```

However, the classical fit doesn't behave in this way. In order to observe the difference in sensitivity of the robust fit with its classical counterpart, we will use package `gam` to estimate the additive functions under both settings: with the original dataset and with the one with the contaminated responses.

```{r gam, warning=FALSE}
library(gam)
dataset <- as.data.frame(cbind(y,X))
colnames(dataset) <- c("y","x1","x2","x3","x4","x5","x6","x7","x8","x9","x10")
fit.gam <- gam(y ~ lo(x1, span=1.62)+lo(x2, span=0.58)+lo(x3, span=0.15)+lo(x4, span=0.08)+
                 lo(x5, span=0.46)+lo(x6, span=0.40)+lo(x7, span=0.30)+lo(x8, span=0.09)+
                 lo(x9, span=0.58)+lo(x10, span=0.45), data=dataset) #gam(y ~ lo(x1, span=2.16)+lo(x2, span=0.78)+lo(x3, span=0.20)+lo(x4, span=0.12)+ lo(x5, span=0.62)+lo(x6, span=0.54)+lo(x7, span=0.40)+lo(x8, span=0.12)+ lo(x9, span=0.77)+lo(x10, span=0.60) , data=dataset)
fits <- predict(fit.gam, type='terms')
dataset.new <- as.data.frame(cbind(ynew,X))
colnames(dataset.new) <- c("ynew","x1","x2","x3","x4","x5","x6","x7","x8","x9","x10")
fit.gam.new <- gam(ynew ~  lo(x1, span=1.62)+lo(x2, span=0.58)+lo(x3, span=0.15)+
                     lo(x4, span=0.08)+lo(x5, span=0.46)+lo(x6, span=0.40)+lo(x7, span=0.30)+
                     lo(x8, span=0.09)+ lo(x9, span=0.58)+lo(x10, span=0.45), data=dataset.new)
fits.new <- predict(fit.gam.new, type='terms')
```

Finally, the estimated curves with the classical fits are shown in the following plots. In orange and dashed lines the classical approach using the original dataset and in purple but solid lines the classical fit computed with the contaminated data.
```{r gamplots}
for(j in 1:10) {
  oo <- order(X[,j])
  name.x <- bquote(paste('x')[.(j)])
  name.y <- bquote(paste(hat('g')[.(j)]))
  plot(X[oo,j], fits.new[oo,j], type="l", lwd=5, col='purple', lty=1, 
       xlab=name.x, ylab=name.y)
  lines(X[oo,j], fits[oo,j], lwd=5, col='darkorange2', lty=2)
}
```


SI QUISIERAMOS HACER ALGO COMO COMPARAR EL CLASICO CON EL ROBUSTO EN EL MISMO GRAFICO, TENDRIAMOS EL PROBLEMA DE QUE NO DAN TAN PARECIDOS. ADEMÁS, CON GAM LAS CURVAS NO DAN PARECIDAS, MIENTRAS QUE SI LO SOS UN POCO MAS SI USAMOS EL ESTIMADOR CLASICO DE NUESTRO PAQUETE. SI NO TE GUSTA LO DE LOS OUTLIERS AGREGADOS A MANO, HABRÁ MANERA DE USAR ESTE MISMO EN LUGAR DE USAR EL DE GAM?
